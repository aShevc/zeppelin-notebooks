{
  "paragraphs": [
    {
      "text": "import org.apache.spark.streaming._\nimport sys.process.stringSeqToProcess\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.storage.StorageLevel\nimport net.elodina.collectd.Metric\nimport org.apache.avro.generic.{GenericData, GenericRecord}\nimport org.apache.avro.io._\nimport org.apache.avro.specific.SpecificDatumReader\nimport net.elodina.collectd.ByteKafkaDecoder\nimport net.elodina.collectd.StringKafkaDecoder\nimport scala.collection.JavaConverters._\nimport java.util.UUID\n\ncase class AggregatedMetrics(hostname: String, value: java.lang.Double){}\n\nval kfkssc \u003d new StreamingContext(sc, Seconds(2))\n\nval topics \u003d Set(\"collectd\")\n\nval brokerList \u003d \"broker-0.service.mesosinfra:31250,broker-1.service.mesosinfra:31250,broker-2.service.mesosinfra:31250\"\n\nval kafkaConf \u003d Map(\n    \"metadata.broker.list\" -\u003e brokerList,\n    \"auto.offset.reset\" -\u003e \"largest\"\n)\n\n//TODO: enabling checkpointing fails the job, need to investigate why\n//kfkssc.checkpoint(\"eldn-streaming\")\n\nval kfkMsgs \u003d KafkaUtils.createDirectStream[String, Array[Byte], StringKafkaDecoder, ByteKafkaDecoder](kfkssc, kafkaConf, topics)\n\nkfkMsgs.filter {msg \u003d\u003e\n  msg._1 \u003d\u003d \"cpu\" \n}.map { msg \u003d\u003e\n  val raw \u003d msg._2\n  val decoder \u003d DecoderFactory.get().binaryDecoder(raw, null)\n  val record: GenericRecord \u003d new SpecificDatumReader[GenericRecord](Metric.getClassSchema()).read(null, decoder)\n      (record.get(\"hostname\").asInstanceOf[String], (record.get(\"values\").asInstanceOf[GenericData.Array[java.lang.Double]].asScala.headOption.getOrElse(new java.lang.Double(0.0)), 1))\n}.reduceByKeyAndWindow({ case ((val1, count1), (val2, count2)) \u003d\u003e ((val1 + val2), (count1 + count2))}, Seconds(30))\n.map({ case (hostname, (sum, count)) \u003d\u003e AggregatedMetrics(hostname, sum/count)}).foreachRDD(rdd \u003d\u003e rdd.toDF().registerTempTable(\"aggrcpu\"))\n\nkfkssc.remember(Minutes(1))\n\nkfkssc.start()",
      "dateUpdated": "Dec 23, 2015 11:43:46 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450770349616_-607165241",
      "id": "20151222-074549_1099578945",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import org.apache.spark.streaming._\nimport sys.process.stringSeqToProcess\nimport org.apache.spark.streaming.kafka._\nimport org.apache.spark.storage.StorageLevel\nimport net.elodina.collectd.Metric\nimport org.apache.avro.generic.{GenericData, GenericRecord}\nimport org.apache.avro.io._\nimport org.apache.avro.specific.SpecificDatumReader\nimport net.elodina.collectd.ByteKafkaDecoder\nimport net.elodina.collectd.StringKafkaDecoder\nimport scala.collection.JavaConverters._\nimport java.util.UUID\ndefined class AggregatedMetrics\nkfkssc: org.apache.spark.streaming.StreamingContext \u003d org.apache.spark.streaming.StreamingContext@7cb98d14\ntopics: scala.collection.immutable.Set[String] \u003d Set(collectd)\nbrokerList: String \u003d broker-0.service.mesosinfra:31250,broker-1.service.mesosinfra:31250,broker-2.service.mesosinfra:31250\nkafkaConf: scala.collection.immutable.Map[String,String] \u003d Map(metadata.broker.list -\u003e broker-0.service.mesosinfra:31250,broker-1.service.mesosinfra:31250,broker-2.service.mesosinfra:31250, auto.offset.reset -\u003e largest)\nkfkMsgs: org.apache.spark.streaming.dstream.InputDStream[(String, Array[Byte])] \u003d org.apache.spark.streaming.kafka.DirectKafkaInputDStream@35bade5b\n"
      },
      "dateCreated": "Dec 22, 2015 7:45:49 AM",
      "dateStarted": "Dec 22, 2015 10:13:46 PM",
      "dateFinished": "Dec 22, 2015 10:14:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "kfkssc.stop(false, false)",
      "dateUpdated": "Dec 22, 2015 10:13:18 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450773184120_275105026",
      "id": "20151222-083304_2028803234",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 22, 2015 8:33:04 AM",
      "dateStarted": "Dec 22, 2015 10:13:18 PM",
      "dateFinished": "Dec 22, 2015 10:13:21 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "%dep\nz.load(\"../notebook-deps/collectd-api.jar\")\nz.load(\"org.apache.spark:spark-streaming-kafka_2.10:1.5.1\")",
      "dateUpdated": "Dec 23, 2015 11:42:41 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450785248458_-1267992373",
      "id": "20151222-115408_652035495",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "Must be used before SparkInterpreter (%spark) initialized"
      },
      "dateCreated": "Dec 22, 2015 11:54:08 AM",
      "dateStarted": "Dec 23, 2015 11:42:41 AM",
      "dateFinished": "Dec 23, 2015 11:42:41 AM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450870961884_-894012089",
      "id": "20151223-114241_1374787274",
      "dateCreated": "Dec 23, 2015 11:42:41 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Elodina streaming",
  "id": "2B8JHNC9E",
  "angularObjects": {
    "2B5JNYQG9": [],
    "2B77KRQ7B": [],
    "2B5UBR2T1": [],
    "2B61KFHHB": [],
    "2B71TKRAX": [],
    "2B6N7TAWJ": [],
    "2B5T4Y134": [],
    "2B64GJ4E8": [],
    "2B85P8F63": [],
    "2B59ACF1Z": [],
    "2B64D415T": [],
    "2B86K7D7B": [],
    "2B8736GXY": []
  },
  "config": {},
  "info": {}
}