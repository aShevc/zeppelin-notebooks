{
  "paragraphs": [
    {
      "text": "import java.util.{Locale, Date, Calendar}\nimport _root_.kafka.serializer.StringDecoder\nimport java.text.DateFormat\nimport java.text.ParseException\nimport java.text.SimpleDateFormat\nimport java.util.UUID\nimport net.elodina.collectd.ByteKafkaDecoder\nimport net.elodina.collectd.Metric\nimport net.elodina.collectd.StringKafkaDecoder\nimport org.apache.avro.generic.{GenericData, GenericRecord}\nimport org.apache.avro.io._\nimport org.apache.avro.specific.SpecificDatumReader\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\nimport scala.collection.JavaConverters._\nimport scala.util.Try\nimport scala.util.matching.Regex\nimport sys.process.stringSeqToProcess\n\n// {{{YCSB\n\ncase class YcsbStat(timestamp: Long, host: String, operation: String, measurement: String, value: Double \u003d 0)\n\nclass YcsbStatParser {\n\n  val msgRe \u003d \"^\\\\[(OVERALL|CLEANUP|READ|READ-MODIFY-WRITE|UPDATE)\\\\], ([^,]+), (.*)$\".r\n\n  def parseMsg(msg: String): Option[(String, String, Double)] \u003d {\n    msg match {\n      case msgRe(op, k, v) \u003d\u003e\n        Some((op, k, java.lang.Double.parseDouble(v)))\n      case _ \u003d\u003e\n        None\n    }\n  }\n\n  def apply(text: String): Option[YcsbStat] \u003d try {\n    if (text \u003d\u003d null || text.isEmpty) return None\n    val line \u003d text.stripLineEnd.trim\n    for {\n      (header, msg) \u003c- parseHeader(line)\n      (operation, measurement, value) \u003c- parseMsg(msg.trim)\n    } yield YcsbStat(header.ts, header.host, operation, measurement, value)\n  } catch {\n    case e: Throwable \u003d\u003e\n      e.printStackTrace()\n      None\n  }\n\n  case class Header(ts: Long, host: String, tag: String)\n\n  val logLine \u003d \"^\u003c(\\\\d+)\u003e(\\\\w+ \\\\d{2} \\\\d{2}:\\\\d{2}:\\\\d{2}) ([^ ]+) ([^:]+): (.*)$\".r // \"\u003c(?pri)\u003e(?month) ?day ?hours:?minutes:?seconds ?host ?tag: ?rest\"\n\n  def parseHeader(text: String): Option[(Header, String)] \u003d {\n\n    if (!text.startsWith(\"\u003c\")) {\n      None\n    } else {\n\n      text match {\n        case logLine(priority, date, host, tag, rest) \u003d\u003e\n          for { ts \u003c- parseDate(date) } yield (Header(ts, host, tag), rest)\n        case _ \u003d\u003e\n          println(\"WARN: unable to parse \" + text)\n          None\n      }\n    }\n  }\n\n  def parseDate(date: String): Option[Long] \u003d try {\n    val d1 \u003d date + \" \" + Calendar.getInstance().get(Calendar.YEAR).toString\n    val parse: Date \u003d dateFormat.parse(d1)\n    Some(parse.getTime)\n  } catch {\n    case e: Throwable \u003d\u003e\n      e.printStackTrace()\n      None\n  }\n\n  val dateFormat: DateFormat \u003d new SimpleDateFormat(\"MMM dd kk:mm:ss yyyy\", Locale.ENGLISH)\n}\n\n// }}} \n\ncase class AggregatedMetrics(hostname: String, value: java.lang.Double){}\n\nval kfkssc \u003d new StreamingContext(sc, Seconds(2))\n\nval topics \u003d Set(\"collectd\")\n\nval brokerList \u003d \"broker-0.service.mesosinfra:31250,broker-1.service.mesosinfra:31250,broker-2.service.mesosinfra:31250\"\n\nval kafkaConf \u003d Map(\n    \"metadata.broker.list\" -\u003e brokerList,\n    \"auto.offset.reset\" -\u003e \"largest\"\n)\n\n//TODO: enabling checkpointing fails the job, need to investigate why\n//kfkssc.checkpoint(\"eldn-streaming\")\n\nval kfkMsgs \u003d KafkaUtils.createDirectStream[String, Array[Byte], StringKafkaDecoder, ByteKafkaDecoder](kfkssc, kafkaConf, topics)\n\nkfkMsgs.filter {msg \u003d\u003e\n  msg._1 \u003d\u003d \"cpu\" \n}.map { msg \u003d\u003e\n  val raw \u003d msg._2\n  val decoder \u003d DecoderFactory.get().binaryDecoder(raw, null)\n  val record: GenericRecord \u003d new SpecificDatumReader[GenericRecord](Metric.getClassSchema()).read(null, decoder)\n      (record.get(\"hostname\").asInstanceOf[String], (record.get(\"values\").asInstanceOf[GenericData.Array[java.lang.Double]].asScala.headOption.getOrElse(new java.lang.Double(0.0)), 1))\n}.reduceByKeyAndWindow({ case ((val1, count1), (val2, count2)) \u003d\u003e ((val1 + val2), (count1 + count2))}, Seconds(30))\n.map({ case (hostname, (sum, count)) \u003d\u003e AggregatedMetrics(hostname, sum/count)}).foreachRDD(rdd \u003d\u003e rdd.toDF().registerTempTable(\"aggrcpu\"))\n\n// {{{YCSB\n\nval syslogs \u003d KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](kfkssc, kafkaConf, Set(\"syslogs\"))\nsyslogs\n  .map(_._2)\n  .flatMap(x \u003d\u003e (new YcsbStatParser).apply(x)).window(Minutes(15), Minutes(1))\n  .foreachRDD(rdd \u003d\u003e rdd.toDF().registerTempTable(\"ycsbMetrics\"))\n\n// }}}\n\nkfkssc.remember(Minutes(1))\n\nkfkssc.start()",
      "dateUpdated": "Dec 24, 2015 12:29:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450770349616_-607165241",
      "id": "20151222-074549_1099578945",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.util.{Locale, Date, Calendar}\nimport _root_.kafka.serializer.StringDecoder\nimport java.text.DateFormat\nimport java.text.ParseException\nimport java.text.SimpleDateFormat\nimport java.util.UUID\nimport net.elodina.collectd.ByteKafkaDecoder\nimport net.elodina.collectd.Metric\nimport net.elodina.collectd.StringKafkaDecoder\nimport org.apache.avro.generic.{GenericData, GenericRecord}\nimport org.apache.avro.io._\nimport org.apache.avro.specific.SpecificDatumReader\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.kafka._\nimport scala.collection.JavaConverters._\nimport scala.util.Try\nimport scala.util.matching.Regex\nimport sys.process.stringSeqToProcess\ndefined class YcsbStat\ndefined class YcsbStatParser\ndefined class AggregatedMetrics\nkfkssc: org.apache.spark.streaming.StreamingContext \u003d org.apache.spark.streaming.StreamingContext@270dd23c\ntopics: scala.collection.immutable.Set[String] \u003d Set(collectd)\nbrokerList: String \u003d broker-0.service.mesosinfra:31250,broker-1.service.mesosinfra:31250,broker-2.service.mesosinfra:31250\nkafkaConf: scala.collection.immutable.Map[String,String] \u003d Map(metadata.broker.list -\u003e broker-0.service.mesosinfra:31250,broker-1.service.mesosinfra:31250,broker-2.service.mesosinfra:31250, auto.offset.reset -\u003e largest)\nkfkMsgs: org.apache.spark.streaming.dstream.InputDStream[(String, Array[Byte])] \u003d org.apache.spark.streaming.kafka.DirectKafkaInputDStream@c3ba9a1\nsyslogs: org.apache.spark.streaming.dstream.InputDStream[(String, String)] \u003d org.apache.spark.streaming.kafka.DirectKafkaInputDStream@4a1c92ac\n"
      },
      "dateCreated": "Dec 22, 2015 7:45:49 AM",
      "dateStarted": "Dec 24, 2015 12:29:13 PM",
      "dateFinished": "Dec 24, 2015 12:29:23 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "kfkssc.stop(false, false)",
      "dateUpdated": "Dec 24, 2015 12:26:36 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450773184120_275105026",
      "id": "20151222-083304_2028803234",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": ""
      },
      "dateCreated": "Dec 22, 2015 8:33:04 AM",
      "dateStarted": "Dec 24, 2015 12:26:36 PM",
      "dateFinished": "Dec 24, 2015 12:26:37 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1450870961884_-894012089",
      "id": "20151223-114241_1374787274",
      "dateCreated": "Dec 23, 2015 11:42:41 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "Elodina streaming",
  "id": "2B8JHNC9E",
  "angularObjects": {
    "2B5JNYQG9": [],
    "2B77KRQ7B": [],
    "2B5UBR2T1": [],
    "2B61KFHHB": [],
    "2B71TKRAX": [],
    "2B6N7TAWJ": [],
    "2B5T4Y134": [],
    "2B64GJ4E8": [],
    "2B85P8F63": [],
    "2B59ACF1Z": [],
    "2B64D415T": [],
    "2B86K7D7B": [],
    "2B8736GXY": []
  },
  "config": {},
  "info": {}
}